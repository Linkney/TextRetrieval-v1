8
1
0
2
c
e
D
5
]
V
C
.
s
c
[
2
v
7
8
1
1
0
.
2
1
8
1
:
v
i
X
r
a
Bag of Tricks for Image Classiﬁcation with Convolutional Neural Networks
Tong He
Zhi Zhang
Hang Zhang
Zhongyue Zhang
Junyuan Xie Mu Li
{htong,zhiz,hzaws,zhongyue,junyuanx,mli}@amazon.com
Amazon Web Services
Abstract
Much of the recent progress made in image classiﬁcation
research can be credited to training procedure reﬁnements,
such as changes in data augmentations and optimization
methods. In the literature, however, most reﬁnements are ei-
ther brieﬂy mentioned as implementation details or only vis-
ible in source code. In this paper, we will examine a collec-
tion of such reﬁnements and empirically evaluate their im-
pact on the ﬁnal model accuracy through ablation study. We
will show that, by combining these reﬁnements together, we
are able to improve various CNN models signiﬁcantly. For
example, we raise ResNet-50’s top-1 validation accuracy
from 75.3% to 79.29% on ImageNet. We will also demon-
strate that improvement on image classiﬁcation accuracy
leads to better transfer learning performance in other ap-
plication domains such as object detection and semantic
segmentation.
1. Introduction
Since the introduction of AlexNet [15] in 2012, deep
convolutional neural networks have become the dominat-
ing approach for image classiﬁcation. Various new architec-
tures have been proposed since then, including VGG [24],
NiN [16], Inception [1], ResNet [9], DenseNet [13], and
NASNet [34]. At the same time, we have seen a steady
trend of model accuracy improvement. For example, the
top-1 validation accuracy on ImageNet [23] has been raised
from 62.5% (AlexNet) to 82.7% (NASNet-A).
However, these advancements did not solely come from
improved model architecture. Training procedure reﬁne-
ments, including changes in loss functions, data preprocess-
ing, and optimization methods also played a major role. A
large number of such reﬁnements has been proposed in the
past years, but has received relatively less attention. In the
literature, most were only brieﬂy mentioned as implemen-
tation details while others can only be found in source code.
In this paper, we will examine a collection of training
Model
ResNet-50 [9]
ResNeXt-50 [27]
SE-ResNet-50 [12]
SE-ResNeXt-50 [12]
DenseNet-201 [13]
ResNet-50 + tricks (ours)
FLOPs
3.9 G
4.2 G
3.9 G
4.3 G
4.3 G
4.3 G
top-1
75.3
77.8
76.71
78.90
77.42
79.29
top-5
92.2
-
93.38
94.51
93.66
94.63
Table 1: Computational costs and validation accuracy of
various models. ResNet, trained with our “tricks”, is able
to outperform newer and improved architectures trained
with standard pipeline.
procedure and model architecture reﬁnements that improve
model accuracy but barely change computational complex-
ity. Many of them are minor “tricks” like modifying the
stride size of a particular convolution layer or adjusting
learning rate schedule. Collectively, however, they make a
big difference. We will evaluate them on multiple network
architectures and datasets and report their impact to the ﬁnal
model accuracy.
Our empirical evaluation shows that several tricks lead
to signiﬁcant accuracy improvement and combining them
together can further boost the model accuracy. We com-
pare ResNet-50, after applying all tricks, to other related
networks in Table 1. Note that these tricks raises ResNet-
50’s top-1 validation accuracy from 75.3% to 79.29% on
ImageNet. It also outperforms other newer and improved
network architectures, such as SE-ResNeXt-50.
In addi-
tion, we show that our approach can generalize to other net-
works (Inception V3 [1] and MobileNet [11]) and datasets
(Place365 [32]). We further show that models trained with
our tricks bring better transfer learning performance in other
application domains such as object detection and semantic
segmentation.
Paper Outline. We ﬁrst set up a baseline training proce-
dure in Section 2, and then discuss several tricks that are
1
Algorithm 1 Train a neural network with mini-batch
stochastic gradient descent.
initialize(net)
for epoch = 1, . . . , K do
for batch = 1, . . . , #images/b do
images ← uniformly random sample b images
X, y ← preprocess(images)
z ← forward(net, X )
(cid:96) ← loss(z , y )
grad ← backward((cid:96))
update(net, grad)
end for
end for
useful for efﬁcient training on new hardware in Section 3. In
Section 4 we review three minor model architecture tweaks
for ResNet and propose a new one. Four additional train-
ing procedure reﬁnements are then discussed in Section 5.
At last, we study if these more accurate models can help
transfer learning in Section 6.
Our model implementations and training scripts are pub-
licly available in GluonCV 1 .
2. Training Procedures
The template of training a neural network with mini-
batch stochastic gradient descent is shown in Algorithm 1.
In each iteration, we randomly sample b images to com-
pute the gradients and then update the network parameters.
It stops after K passes through the dataset. All functions
and hyper-parameters in Algorithm 1 can be implemented
in many different ways. In this section, we ﬁrst specify a
baseline implementation of Algorithm 1.
Model
ResNet-50 [9]
Inception-V3 [26]
MobileNet [11]
Baseline
Top-1
Top-5
75.87
92.70
77.32
93.43
69.03
88.71
Reference
Top-1
Top-5
75.3
92.2
78.8
94.4
70.6
-
Table 2: Validation accuracy of reference implementa-
tions and our baseline. Note that the numbers for Incep-
tion V3 are obtained with 299-by-299 input images.
6. Normalize RGB channels by subtracting 123.68,
116.779, 103.939 and dividing by 58.393, 57.12,
57.375, respectively.
During validation, we resize each image’s shorter edge
to 256 pixels while keeping its aspect ratio. Next, we crop
out the 224-by-224 region in the center and normalize RGB
channels similar to training. We do not perform any random
augmentations during validation.
The weights of both convolutional and fully-connected
drawn from [−a, a], where a = (cid:112)6/(din + dout ). Here
layers are initialized with the Xavier algorithm [6]. In par-
ticular, we set the parameter to random values uniformly
din and dout are the input and output channel sizes, respec-
tively. All biases are initialized to 0. For batch normaliza-
tion layers, γ vectors are initialized to 1 and β vectors to
0.
Nesterov Accelerated Gradient (NAG) descent [20] is
used for training. Each model is trained for 120 epochs on
8 Nvidia V100 GPUs with a total batch size of 256. The
learning rate is initialized to 0.1 and divided by 10 at the
30th, 60th, and 90th epochs.
2.1. Baseline Training Procedure
2.2. Experiment Results
We follow a widely used implementation [8] of ResNet
as our baseline. The preprocessing pipelines between train-
ing and validation are different. During training, we per-
form the following steps one-by-one:
1. Randomly sample an image and decode it into 32-bit
ﬂoating point raw pixel values in [0, 255].
2. Randomly crop a rectangular region whose aspect ratio
is randomly sampled in [3/4, 4/3] and area randomly
sampled in [8%, 100%], then resize the cropped region
into a 224-by-224 square image.
3. Flip horizontally with 0.5 probability.
4. Scale hue, saturation, and brightness with coefﬁcients
uniformly drawn from [0.6, 1.4].
5. Add PCA noise with a coefﬁcient sampled from a nor-
mal distribution N (0, 0.1).
1 https://github.com/dmlc/gluon- cv
We evaluate three CNNs: ResNet-50 [9], Inception-
V3 [1], and MobileNet [11]. For Inception-V3 we resize the
input images into 299x299. We use the ISLVRC2012 [23]
dataset, which has 1.3 million images for training and 1000
classes. The validation accuracies are shown in Table 2. As
can be seen, our ResNet-50 results are slightly better than
the reference results, while our baseline Inception-V3 and
MobileNet are slightly lower in accuracy due to different
training procedure.
3. Efﬁcient Training
Hardware, especially GPUs, has been rapidly evolving
in recent years. As a result, the optimal choices for many
performance related trade-offs have changed. For example,
it is now more efﬁcient to use lower numerical precision and
larger batch sizes during training. In this section, we review
various techniques that enable low precision and large batch
training without sacriﬁcing model accuracy. Some tech-
niques can even improve both accuracy and training speed.
3.1. Large-batch training
Mini-batch SGD groups multiple samples to a mini-
batch to increase parallelism and decrease communication
costs. Using large batch size, however, may slow down
the training progress. For convex problems, convergence
rate decreases as batch size increases. Similar empirical re-
sults have been reported for neural networks [25]. In other
words, for the same number of epochs, training with a large
batch size results in a model with degraded validation accu-
racy compared to the ones trained with smaller batch sizes.
Multiple works [7, 14] have proposed heuristics to solve
this issue.
In the following paragraphs, we will examine
four heuristics that help scale the batch size up for single
machine training.
Linear scaling learning rate.
In mini-batch SGD, gradi-
ent descending is a random process because the examples
are randomly selected in each batch. Increasing the batch
size does not change the expectation of the stochastic gra-
dient but reduces its variance. In other words, a large batch
size reduces the noise in the gradient, so we may increase
the learning rate to make a larger progress along the op-
posite of the gradient direction. Goyal et al. [7] reports
that linearly increasing the learning rate with the batch size
works empirically for ResNet-50 training. In particular, if
we follow He et al. [9] to choose 0.1 as the initial learn-
ing rate for batch size 256, then when changing to a larger
batch size b, we will increase the initial learning rate to
0.1 × b/256.
Learning rate warmup. At the beginning of the training,
all parameters are typically random values and therefore far
away from the ﬁnal solution. Using a too large learning rate
may result in numerical instability. In the warmup heuristic,
we use a small learning rate at the beginning and then switch
back to the initial learning rate when the training process
is stable [9]. Goyal et al. [7] proposes a gradual warmup
strategy that increases the learning rate from 0 to the initial
learning rate linearly. In other words, assume we will use
the ﬁrst m batches (e.g. 5 data epochs) to warm up, and the
initial learning rate is η , then at batch i, 1 ≤ i ≤ m, we will
set the learning rate to be iη/m.
Zero γ . A ResNet network consists of multiple residual
blocks, each block consists of several convolutional lay-
ers. Given input x, assume block(x) is the output for the
last layer in the block, this residual block then outputs
x + block(x). Note that the last layer of a block could
be a batch normalization (BN) layer. The BN layer ﬁrst
standardizes its input, denoted by ˆx, and then performs a
scale transformation γ ˆx + β . Both γ and β are learnable
parameters whose elements are initialized to 1s and 0s, re-
spectively. In the zero γ initialization heuristic, we initialize
γ = 0 for all BN layers that sit at the end of a residual block.
Therefore, all residual blocks just return their inputs, mim-
ics network that has less number of layers and is easier to
train at the initial stage.
No bias decay. The weight decay is often applied to all
learnable parameters including both weights and bias. It’s
equivalent to applying an L2 regularization to all parame-
ters to drive their values towards 0. As pointed out by Jia et
al. [14], however, it’s recommended to only apply the reg-
ularization to weights to avoid overﬁtting. The no bias de-
cay heuristic follows this recommendation, it only applies
the weight decay to the weights in convolution and fully-
connected layers. Other parameters, including the biases
and γ and β in BN layers, are left unregularized.
Note that LARS [4] offers layer-wise adaptive learning
rate and is reported to be effective for extremely large batch
sizes (beyond 16K). While in this paper we limit ourselves
to methods that are sufﬁcient for single machine training,
in which case a batch size no more than 2K often leads to
good system efﬁciency.
3.2. Low-precision training
Neural networks are commonly trained with 32-bit ﬂoat-
ing point (FP32) precision. That is, all numbers are stored in
FP32 format and both inputs and outputs of arithmetic oper-
ations are FP32 numbers as well. New hardware, however,
may have enhanced arithmetic logic unit for lower precision
data types. For example, the previously mentioned Nvidia
V100 offers 14 TFLOPS in FP32 but over 100 TFLOPS in
FP16. As in Table 3, the overall training speed is acceler-
ated by 2 to 3 times after switching from FP32 to FP16 on
V100.
Despite the performance beneﬁt, a reduced precision has
a narrower range that makes results more likely to be out-of-
range and then disturb the training progress. Micikevicius et
al. [19] proposes to store all parameters and activations in
FP16 and use FP16 to compute gradients. At the same time,
all parameters have an copy in FP32 for parameter updat-
ing. In addition, multiplying a scalar to the loss to better
align the range of the gradient into FP16 is also a practical
solution.
3.3. Experiment Results
The evaluation results for ResNet-50 are shown in Ta-
ble 3. Compared to the baseline with batch size 256 and
FP32, using a larger 1024 batch size and FP16 reduces the
training time for ResNet-50 from 13.3-min per epoch to 4.4-
min per epoch. In addition, by stacking all heuristics for
Figure 1: The architecture of ResNet-50. The convolution
kernel size, output channel size and stride size (default is 1)
are illustrated, similar for pooling layers.
large-batch training, the model trained with 1024 batch size
and FP16 even slightly increased 0.5% top-1 accuracy com-
pared to the baseline model.
The ablation study of all heuristics is shown in Table 4.
Increasing batch size from 256 to 1024 by linear scaling
learning rate alone leads to a 0.9% decrease of the top-1
accuracy while stacking the rest three heuristics bridges the
gap. Switching from FP32 to FP16 at the end of training
does not affect the accuracy.
4. Model Tweaks
A model tweak is a minor adjustment to the network ar-
chitecture, such as changing the stride of a particular convo-
lution layer. Such a tweak often barely changes the compu-
tational complexity but might have a non-negligible effect
on the model accuracy. In this section, we will use ResNet
as an example to investigate the effects of model tweaks.
4.1. ResNet Architecture
We will brieﬂy present the ResNet architecture, espe-
cially its modules related to the model tweaks. For detailed
information please refer to He et al. [9]. A ResNet network
consists of an input stem, four subsequent stages and a ﬁnal
output layer, which is illustrated in Figure 1. The input stem
has a 7 × 7 convolution with an output channel of 64 and a
stride of 2, followed by a 3 × 3 max pooling layer also with
a stride of 2. The input stem reduces the input width and
height by 4 times and increases its channel size to 64.
Starting from stage 2, each stage begins with a down-
sampling block, which is then followed by several residual
blocks. In the downsampling block, there are path A and
(a) ResNet-B
(b) ResNet-C
(c) ResNet-D
Figure 2: Three ResNet tweaks. ResNet-B modiﬁes the
downsampling block of Resnet. ResNet-C further modiﬁes
the input stem. On top of that, ResNet-D again modiﬁes the
downsampling block.
path B. Path A has three convolutions, whose kernel sizes
are 1 × 1, 3 × 3 and 1 × 1, respectively. The ﬁrst convolution
has a stride of 2 to halve the input width and height, and the
last convolution’s output channel is 4 times larger than the
previous two, which is called the bottleneck structure. Path
B uses a 1 × 1 convolution with a stride of 2 to transform the
input shape to be the output shape of path A, so we can sum
outputs of both paths to obtain the output of the downsam-
pling block. A residual block is similar to a downsampling
block except for only using convolutions with a stride of 1.
One can vary the number of residual blocks in each stage
to obtain different ResNet models, such as ResNet-50 and
ResNet-152, where the number presents the number of con-
volutional layers in the network.
4.2. ResNet Tweaks
Next, we revisit two popular ResNet tweaks, we call
them ResNet-B and ResNet-C, respectively. We propose
a new model tweak ResNet-D afterwards.
ResNet-B. This tweak ﬁrst appeared in a Torch imple-
mentation of ResNet [8] and then adopted by multiple
works [7, 12, 27]. It changes the downsampling block of
ResNet. The observation is that the convolution in path A
ignores three-quarters of the input feature map because it
uses a kernel size 1×1 with a stride of 2. ResNet-B switches
the strides size of the ﬁrst two convolutions in path A, as
shown in Figure 2a, so no information is ignored. Because
the second convolution has a kernel size 3 × 3, the output
shape of path A remains unchanged.
ResNet-C. This tweak was proposed in Inception-v2 [26]
originally, and it can be found on the implementations
Model
Efﬁcient
Time/epoch
Top-1
ResNet-50
Inception-V3
MobileNet
4.4 min
8 min
3.7 min
76.21
77.50
71.90
Top-5
92.97
93.60
90.47
Baseline
Time/epoch
Top-1
13.3 min
75.87
19.8 min
77.32
6.2 min
69.03
Top-5
92.70
93.43
88.71
Table 3: Comparison of the training time and validation accuracy for ResNet-50 between the baseline (BS=256 with FP32)
and a more hardware efﬁcient setting (BS=1024 with FP16).
Heuristic
Linear scaling
+ LR warmup
+ Zero γ
+ No bias decay
+ FP16
BS=256
Top-1
Top-5
75.87
92.70
76.03
92.81
76.19
93.03
76.16
92.97
76.15
93.09
BS=1024
Top-1
Top-5
75.17
92.54
75.93
92.84
76.37
92.96
76.03
92.86
76.21
92.97
Table 4: The breakdown effect for each effective training
heuristic on ResNet-50.
of other models, such as SENet [12], PSPNet [31],
DeepLabV3 [1], and ShufﬂeNetV2 [21]. The observation
is that the computational cost of a convolution is quadratic
to the kernel width or height. A 7 × 7 convolution is 5.4
times more expensive than a 3 × 3 convolution. So this
tweak replacing the 7 × 7 convolution in the input stem
with three conservative 3 × 3 convolutions, which is shown
in Figure 2b, with the ﬁrst and second convolutions have
their output channel of 32 and a stride of 2, while the last
convolution uses a 64 output channel.
ResNet-D.
Inspired by ResNet-B, we note that the 1 × 1
convolution in the path B of the downsampling block also
ignores 3/4 of input feature maps, we would like to modify
it so no information will be ignored. Empirically, we found
adding a 2×2 average pooling layer with a stride of 2 before
the convolution, whose stride is changed to 1, works well
in practice and impacts the computational cost little. This
tweak is illustrated in Figure 2c.
4.3. Experiment Results
We evaluate ResNet-50 with the three tweaks and set-
tings described in Section 3, namely the batch size is 1024
and precision is FP16. The results are shown in Table 5.
Suggested by the results, ResNet-B receives more infor-
mation in path A of the downsampling blocks and improves
validation accuracy by around 0.5% compared to ResNet-
50. Replacing the 7 × 7 convolution with three 3 × 3 ones
gives another 0.2% improvement. Taking more information
in path B of the downsampling blocks improves the vali-
Model
ResNet-50
ResNet-50-B
ResNet-50-C
ResNet-50-D
#params
25 M
25 M
25 M
25 M
FLOPs
3.8 G
4.1 G
4.3 G
4.3 G
Top-1
76.21
76.66
76.87
77.16
Top-5
92.97
93.28
93.48
93.52
Table 5: Compare ResNet-50 with three model tweaks on
model size, FLOPs and ImageNet validation accuracy.
In total, ResNet-50-D
dation accuracy by another 0.3%.
improves ResNet-50 by 1%.
On the other hand, these four models have the same
model size. ResNet-D has the largest computational cost,
but its difference compared to ResNet-50 is within 15% in
terms of ﬂoating point operations. In practice, we observed
ResNet-50-D is only 3% slower in training throughput com-
pared to ResNet-50.
5. Training Reﬁnements
In this section, we will describe four training reﬁnements
that aim to further improve the model accuracy.
5.1. Cosine Learning Rate Decay
Learning rate adjustment is crucial to the training. Af-
ter the learning rate warmup described in Section 3.1, we
typically steadily decrease the value from the initial learn-
ing rate. The widely used strategy is exponentially decaying
the learning rate. He et al. [9] decreases rate at 0.1 for ev-
ery 30 epochs, we call it “step decay”. Szegedy et al. [26]
decreases rate at 0.94 for every two epochs.
In contrast to it, Loshchilov et al. [18] propose a cosine
annealing strategy. An simpliﬁed version is decreasing the
learning rate from the initial value to 0 by following the
cosine function. Assume the total number of batches is T
(the warmup stage is ignored), then at batch t, the learning
rate ηt is computed as:
1 + cos
η ,
(1)
(cid:18)
ηt =
1
2
(cid:19)(cid:19)
(cid:18) tπ
T
where η is the initial learning rate. We call this scheduling
as “cosine” decay.
(a) Learning Rate Schedule
(b) Validation Accuracy
Figure 3: Visualization of learning rate schedules with
warm-up. Top: cosine and step schedules for batch size
1024. Bottom: Top-1 validation accuracy curve with regard
to the two schedules.
The comparison between step decay and cosine decay
are illustrated in Figure 3a. As can be seen, the cosine decay
decreases the learning rate slowly at the beginning, and then
becomes almost linear decreasing in the middle, and slows
down again at the end. Compared to the step decay, the
cosine decay starts to decay the learning since the beginning
but remains large until step decay reduces the learning rate
by 10x, which potentially improves the training progress.
5.2. Label Smoothing
The last layer of a image classiﬁcation network is often a
fully-connected layer with a hidden size being equal to the
number of labels, denote by K , to output the predicted con-
ﬁdence scores. Given an image, denote by zi the predicted
score for class i. These scores can be normalized by the
softmax operator to obtain predicted probabilities. Denote
by q the output of the softmax operator q = softmax(z ), the
probability for class i, qi , can be computed by:
(cid:80)K
exp(zi )
j=1 exp(zj )
.
qi =
(2)
It’s easy to see qi > 0 and (cid:80)K
i=1 qi = 1, so q is a valid
probability distribution.
On the other hand, assume the true label of this image
is y , we can construct a truth probability distribution to be
pi = 1 if i = y and 0 otherwise. During training, we mini-
mize the negative cross entropy loss
(cid:96)(p, q) = − K(cid:88)
i=1
qi log pi
(3)
(cid:17)
i=1 exp(zi )
−zy + log
(cid:16)(cid:80)K
to update model parameters to make these two probabil-
ity distributions similar to each other. In particular, by the
way how p is constructed, we know (cid:96)(p, q) = − log py =
. The optimal solution is z ∗
inf while keeping others small enough. In other words, it
encourages the output scores dramatically distinctive which
potentially leads to overﬁtting.
The idea of label smoothing was ﬁrst proposed to train
Inception-v2 [26]. It changes the construction of the true
probability to
y =
qi =
1 − ε
ε/(K − 1)
if i = y ,
otherwise,
(4)
(cid:40)
(cid:40)
where ε is a small constant. Now the optimal solution
becomes
z ∗
i =
log((K − 1)(1 − ε)/ε) + α
α
if i = y ,
otherwise,
(5)
where α can be an arbitrary real number. This encour-
ages a ﬁnite output from the fully-connected layer and can
generalize better.
When ε = 0, the gap log((K − 1)(1 − ε)/ε) will be
∞ and as ε increases, the gap decreases. Speciﬁcally when
ε = (K − 1)/K , all optimal z ∗
i will be identical. Figure 4a
shows how the gap changes as we move ε, given K = 1000
for ImageNet dataset.
We empirically compare the output value from two
ResNet-50-D models that are trained with and without la-
bel smoothing respectively and calculate the gap between
the maximum prediction value and the average of the rest.
Under ε = 0.1 and K = 1000, the theoretical gap is around
9.1. Figure 4b demonstrate the gap distributions from the
two models predicting over the validation set of ImageNet.
It is clear that with label smoothing the distribution centers
at the theoretical value and has fewer extreme values.
5.3. Knowledge Distillation
In knowledge distillation [10], we use a teacher model
to help train the current model, which is called the student
model. The teacher model is often a pre-trained model with
higher accuracy, so by imitation, the student model is able
to improve its own accuracy while keeping the model com-
plexity the same. One example is using a ResNet-152 as the
teacher model to help training ResNet-50.
During training, we add a distillation loss to penalize
the difference between the softmax outputs from the teacher
model and the learner model. Given an input, assume p is
the true probability distribution, and z and r are outputs of
the last fully-connected layer of the student model and the
teacher model, respectively. Remember previously we use a
(a) Theoretical gap
(b) Empirical gap from ImageNet validation set
Figure 4: Visualization of the effectiveness of label smooth-
ing on ImageNet. Top: theoretical gap between z ∗
p and oth-
ers decreases when increasing ε. Bottom: The empirical
distributions of the gap between the maximum prediction
and the average of the rest.
negative cross entropy loss (cid:96)(p, softmax(z )) to measure the
difference between p and z , here we use the same loss again
for the distillation. Therefore, the loss is changed to
(cid:96)(p, softmax(z )) + T 2 (cid:96)(softmax(r/T ), softmax(z/T )),
(6)
where T is the temperature hyper-parameter to make the
softmax outputs smoother thus distill the knowledge of la-
bel distribution from teacher’s prediction.
5.4. Mixup Training
In Section 2.1 we described how images are augmented
before training. Here we consider another augmentation
method called mixup [29].
In mixup, each time we ran-
domly sample two examples (xi , yi ) and (xj , yj ). Then we
form a new example by a weighted linear interpolation of
these two examples:
ˆx = λxi + (1 − λ)xj ,
ˆy = λyi + (1 − λ)yj ,
(7)
(8)
where λ ∈ [0, 1] is a random number drawn from the
Beta(α, α) distribution.
In mixup training, we only use
the new example ( ˆx, ˆy).
5.5. Experiment Results
Now we evaluate the four training reﬁnements. We
set ε = 0.1 for label smoothing by following Szegedy et
al. [26]. For the model distillation we use T = 20, specif-
ically a pretrained ResNet-152-D model with both cosine
decay and label smoothing applied is used as the teacher.
In the mixup training, we choose α = 0.2 in the Beta dis-
tribution and increase the number of epochs from 120 to
200 because the mixed examples ask for a longer training
progress to converge better. When combining the mixup
training with distillation, we train the teacher model with
mixup as well.
We demonstrate that the reﬁnements are not only lim-
ited to ResNet architecture or the ImageNet dataset. First,
we train ResNet-50-D, Inception-V3 and MobileNet on Im-
ageNet dataset with reﬁnements. The validation accura-
cies for applying these training reﬁnements one-by-one are
shown in Table 6. By stacking cosine decay, label smooth-
ing and mixup, we have steadily improving ResNet, Incep-
tionV3 and MobileNet models. Distillation works well on
ResNet, however, it does not work well on Inception-V3
and MobileNet. Our interpretation is that the teacher model
is not from the same family of the student, therefore has
different distribution in the prediction, and brings negative
impact to the model.
To support our tricks is transferable to other dataset, we
train a ResNet-50-D model on MIT Places365 dataset with
and without the reﬁnements. Results are reported in Ta-
ble 7. We see the reﬁnements improve the top-5 accuracy
consistently on both the validation and test set.
6. Transfer Learning
Transfer learning is one major down-streaming use case
of trained image classiﬁcation models. In this section, we
will investigate if these improvements discussed so far can
beneﬁt transfer learning. In particular, we pick two impor-
tant computer vision tasks, object detection and semantic
segmentation, and evaluate their performance by varying
base models.
6.1. Object Detection
The goal of object detection is to locate bounding boxes
of objects in an image. We evaluate performance using
PASCAL VOC [3]. Similar to Ren et al. [22], we use union
set of VOC 2007 trainval and VOC 2012 trainval for train-
ing, and VOC 2007 test for evaluation, respectively. We
train Faster-RCNN [22] on this dataset, with reﬁnements
from Detectron [5] such as linear warmup and long train-
ing schedule. The VGG-19 base model in Faster-RCNN
is replaced with various pretrained models in the previous
discussion. We keep other settings the same so the gain is
solely from the base models.
Mean average precision (mAP) results are reported in
Table 8. We can observe that a base model with a higher
validation accuracy leads to a higher mAP for Faster-RNN
in a consistent manner. In particular, the best base model
with accuracy 79.29% on ImageNet leads to the best mAP
Reﬁnements
Efﬁcient
+ cosine decay
+ label smoothing
+ distill w/o mixup
+ mixup w/o distill
+ distill w/ mixup
ResNet-50-D
Top-1
Top-5
77.16
93.52
77.91
93.81
78.31
94.09
78.67
94.36
79.15
94.58
79.29
94.63
Inception-V3
Top-1
Top-5
77.50
93.60
78.19
94.06
78.40
94.13
78.26
94.01
78.77
94.39
MobileNet
Top-1
Top-5
71.90
90.53
72.83
91.00
72.93
91.14
71.97
90.89
73.28
91.30
78.34
94.16
72.51
91.02
Table 6: The validation accuracies on ImageNet for stacking training reﬁnements one by one. The baseline models are
obtained from Section 3.
Model
ResNet-50-D Efﬁcient
ResNet-50-D Best
Val Top-1 Acc Val Top-5 Acc
56.34
86.87
56.70
87.33
Test Top-1 Acc
57.18
57.63
Test Top-5 Acc
87.28
87.82
Table 7: Results on both the validation set and the test set of MIT Places 365 dataset. Prediction are generated as stated
in Section 2.1. ResNet-50-D Efﬁcient refers to ResNet-50-D trained with settings from Section 3, and ResNet-50-D Best
further incorporate cosine scheduling, label smoothing and mixup.
Reﬁnement
B-standard
D-efﬁcient
+ cosine
+ smooth
+ distill w/o mixup
+ mixup w/o distill
+ distill w/ mixup
Top-1 mAP
76.14
77.54
77.16
78.30
77.91
79.23
78.34
80.71
78.67
80.96
79.16
81.10
79.29
81.33
Table 8: Faster-RCNN performance with various pre-
trained base networks evaluated on Pascal VOC.
Reﬁnement
B-standard
D-efﬁcient
+ cosine
+ smooth
+ distill w/o mixup
+ mixup w/o distill
+ mixup w/ distill
Top-1
76.14
77.16
77.91
78.34
78.67
79.16
79.29
PixAcc mIoU
78.08
37.05
78.88
38.88
79.25
39.33
78.64
78.97
78.47
78.72
38.75
38.90
37.99
38.40
Table 9: FCN performance with various base networks eval-
uated on ADE20K.
at 81.33% on VOC, which outperforms the standard model
by 4%.
6.2. Semantic Segmentation
Semantic segmentation predicts the category for every
pixel from the input images. We use Fully Convolutional
Network (FCN) [17] for this task and train models on the
ADE20K [33] dataset. Following PSPNet [31] and Zhang et
al. [30], we replace the base network with various pre-
trained models discussed in previous sections and apply di-
lation network strategy [2, 28] on stage-3 and stage-4. A
fully convolutional decoder is built on top of the base net-
work to make the ﬁnal prediction.
Both pixel accuracy (pixAcc) and mean intersection over
union (mIoU) are reported in Table 9.
In contradiction
to our results on object detection, the cosine learning rate
schedule effectively improves the accuracy of the FCN per-
formance, while other reﬁnements provide suboptimal re-
sults. A potential explanation to the phenomenon is that
semantic segmentation predicts in the pixel level. While
models trained with label smoothing, distillation and mixup
favor soften labels, blurred pixel-level information may be
blurred and degrade overall pixel-level accuracy.
7. Conclusion
In this paper, we survey a dozen tricks to train deep
convolutional neural networks to improve model accuracy.
These tricks introduce minor modiﬁcations to the model
architecture, data preprocessing, loss function, and learn-
ing rate schedule. Our empirical results on ResNet-50,
Inception-V3 and MobileNet indicate that these tricks im-
prove model accuracy consistently. More excitingly, stack-
ing all of them together leads to a signiﬁcantly higher accu-
racy. In addition, these improved pre-trained models show
strong advantages in transfer learning, which improve both
object detection and semantic segmentation. We believe the
beneﬁts can extend to broader domains where classiﬁcation
base models are favored.
References
[1] L. Chen, G. Papandreou, F. Schroff, and H. Adam. Re-
thinking atrous convolution for semantic image segmenta-
tion. CoRR, abs/1706.05587, 2017. 1, 2, 5
[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. IEEE transactions on pattern analysis and ma-
chine intelligence, 40(4):834–848, 2018. 8
[3] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
and A. Zisserman. The PASCAL Visual Object Classes
Challenge 2007 (VOC2007) Results.
http://www.pascal-
network.org/challenges/VOC/voc2007/workshop/index.html.
7
[4] B. Ginsburg, I. Gitman, and Y. You. Large batch training of
convolutional networks with layer-wise adaptive rate scaling.
2018. 3
[5] R. Girshick,
and K. He.
I. Radosavovic, G. Gkioxari, P. Doll ´ar,
Detectron.
https://github.com/
facebookresearch/detectron, 2018. 7
[6] X. Glorot and Y. Bengio. Understanding the difﬁculty of
training deep feedforward neural networks. In Proceedings
of the thirteenth international conference on artiﬁcial intel-
ligence and statistics, pages 249–256, 2010. 2
[7] P. Goyal, P. Doll ´ar, R. B. Girshick, P. Noordhuis,
L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He.
Accurate, large minibatch SGD: training imagenet in 1 hour.
CoRR, abs/1706.02677, 2017. 3, 4
[8] S. Gross and M. Wilber. Training and investigating residual
nets. http://torch.ch/blog/2016/02/04/resnets.html. 2, 4
[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016. 1, 2, 3, 4, 5
[10] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
in a neural network. arXiv preprint arXiv:1503.02531, 2015.
6
[11] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efﬁ-
cient convolutional neural networks for mobile vision appli-
cations. arXiv preprint arXiv:1704.04861, 2017. 1, 2
[12] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation net-
works. arXiv preprint arXiv:1709.01507, 7, 2017. 1, 4, 5
[13] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Wein-
berger. Densely connected convolutional networks. In 2017
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 2261–2269. IEEE, 2017. 1
[14] X. Jia, S. Song, W. He, Y. Wang, H. Rong, F. Zhou, L. Xie,
Z. Guo, Y. Yang, L. Yu, et al. Highly scalable deep learning
training system with mixed-precision: Training imagenet in
four minutes. arXiv preprint arXiv:1807.11205, 2018. 3
[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton.
Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages
1097–1105, 2012. 1
[16] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv
preprint arXiv:1312.4400, 2013. 1
[17] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 3431–3440, 2015. 8
[18] I. Loshchilov and F. Hutter. SGDR: stochastic gradient de-
scent with restarts. CoRR, abs/1608.03983, 2016. 5
[19] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen,
D. Garcia, B. Ginsburg, M. Houston, O. Kuchaev,
G. Venkatesh, et al. Mixed precision training. arXiv preprint
arXiv:1710.03740, 2017. 3
[20] Y. E. Nesterov. A method for solving the convex program-
ming problem with convergence rate o (1/kˆ 2).
In Dokl.
Akad. Nauk SSSR, volume 269, pages 543–547, 1983. 2
[21] H.-T. Z. Ningning Ma, Xiangyu Zhang and J. Sun. Shufﬂenet
v2: Practical guidelines for efﬁcient cnn architecture design.
arXiv preprint arXiv:1807.11164, 2018. 5
[22] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
Advances in neural information processing systems, pages
91–99, 2015. 7
[23] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al.
Imagenet large scale visual recognition challenge.
International Journal of Computer Vision, 115(3):211–252,
2015. 1, 2
[24] K. Simonyan and A. Zisserman.
Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 1
[25] S. L. Smith, P.-J. Kindermans, C. Ying, and Q. V. Le. Don’t
decay the learning rate,
increase the batch size.
arXiv
preprint arXiv:1711.00489, 2017. 3
[26] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision.
CoRR, abs/1512.00567, 2015. 2, 4, 5, 6, 7
[27] S. Xie, R. Girshick, P. Doll ´ar, Z. Tu, and K. He. Aggregated
residual transformations for deep neural networks. In Com-
puter Vision and Pattern Recognition (CVPR), 2017 IEEE
Conference on, pages 5987–5995. IEEE, 2017. 1, 4
[28] F. Yu and V. Koltun. Multi-scale context aggregation by di-
lated convolutions. arXiv preprint arXiv:1511.07122, 2015.
8
[29] H. Zhang, M. Ciss ´e, Y. N. Dauphin, and D. Lopez-
Paz. mixup: Beyond empirical risk minimization. CoRR,
abs/1710.09412, 2017. 7
[30] H. Zhang, K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and
A. Agrawal. Context encoding for semantic segmentation.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018. 8
[31] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene
parsing network. In Computer Vision and Pattern Recogni-
tion (CVPR), 2017 IEEE Conference on, pages 6230–6239.
IEEE, 2017. 5, 8
[32] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba.
Places: A 10 million image database for scene recognition.
IEEE transactions on pattern analysis and machine intelli-
gence, 2017. 1
[33] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Tor-
ralba. Scene parsing through ade20k dataset.
In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2017. 8
[34] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le. Learn-
ing transferable architectures for scalable image recognition.
CoRR, abs/1707.07012, 2017. 1
